# 相机校准

## 目标

在本节中，我们将学习

- 相机造成的失真类型
- 如何找到相机的内在和外在特性
- 如何根据这些属性使图像不失真

## 基础

一些针孔相机会给图像带来明显的失真。两种主要的变形是**径向变形**和**切向变形**。

径向变形会导致直线出现弯曲。距图像中心越远，径向畸变越大。例如，下面显示一个图像，其中棋盘的两个边缘用红线标记。但是，您会看到国际象棋棋盘的边框不是直线，并且与红线不匹配。所有预期的直线都凸出。有关更多详细信息，请访问“ [失真（光学）](http://en.wikipedia.org/wiki/Distortion_%28optics%29) ”。

同样，由于摄像镜头未完全平行于成像平面对齐，因此会发生切向畸变。因此，图像中的某些区域看起来可能比预期的要近。

简而言之，我们需要找到五个参数，称为失真系数，公式如下：
$$
Distortioncoefficients=(k1 k2 p1 p2 k3)
$$
除此之外，我们还需要其他一些信息，例如摄像机的内在和外在参数。内部参数特定于摄像机。它们包括像焦距和光学中心,焦距光学中心可用于创建相机矩阵，该相机矩阵可用于消除由于特定相机镜头而引起的畸变。相机矩阵对于特定相机而言是唯一的，因此一旦计算出，就可以在同一相机拍摄的其他图像上重复使用。它表示为3x3矩阵：
$$
\quad
camera matrix = \begin{bmatrix} f_x & 0 & c_x \\
                      0 & f_y & c_y \\
                      0 & 0 & 1\end{bmatrix}
\quad
$$
外在参数对应于旋转和平移矢量，其将3D点的坐标平移为坐标系。

对于立体声应用，首先需要纠正这些失真。为了找到这些参数，我们必须提供一些定义良好的图案的示例图像（例如国际象棋棋盘）。我们找到一些已经知道相对位置的特定点（例如，国际象棋棋盘中的四角）。我们知道现实世界中这些点的坐标，也知道图像中的坐标，因此我们可以求解失真系数。为了获得更好的结果，我们至少需要10个测试模式。

## 示例代码

如上所述，相机校准至少需要10个测试图案。OpenCV附带了一些国际象棋棋盘的图像（请参见`samples/data/left01.jpg –left14.jpg）`，因此我们将利用这些图像。考虑棋盘的图像。相机校准所需的重要输入数据是3D现实世界点集以及图像中这些点的相应2D坐标。可以从图像中轻松找到2D图像点。（这些图像点是棋盘上两个黑色正方形相互接触的位置）

现实世界中的3D点呢？这些图像是从静态相机拍摄的，而国际象棋棋盘放置在不同的位置和方向。所以我们需要知道（X，Y，Z）价值观。但是为简单起见，我们可以说棋盘在XY平面上保持静止（因此Z始终为0），并且照相机也相应地移动了。这种考虑有助于我们仅找到X，Y值。现在对于X，Y值，我们可以简单地将点传递为（0,0），（1,0），（2,0），...，这表示点的位置。在这种情况下，我们得到的结果将是棋盘正方形的大小比例。但是，如果我们知道正方形大小（例如30毫米），则可以将值传递为（0,0），（30,0），（60,0），...。因此，我们得到的结果以毫米为单位。（在这种情况下，我们不知道正方形尺寸，因为我们没有拍摄那些图像，因此我们以正方形尺寸表示）。

3D点称为**对象点**，而2D图像点称为**图像点**。

### 设定

因此，要在国际象棋棋盘中查找图案，我们可以使用函数`cv.findChessboardCorners()`。我们还需要传递所需的图案，例如8x8网格，5x5网格等。在此示例中，我们使用7x6网格。（通常，棋盘有8x8的正方形和7x7的内部角）。它返回角点和retval，如果获得图案，则为True。这些角将按顺序放置（从左到右，从上到下）

### 也可以看看

此功能可能无法在所有图像中找到所需的图案。因此，一个不错的选择是编写代码，使它启动相机并检查每帧所需的图案。获得图案后，找到角并将其存储在列表中。另外，在阅读下一帧之前请提供一些时间间隔，以便我们可以在不同方向上调整棋盘。继续此过程，直到获得所需数量的良好图案为止。即使在此处提供的示例中，我们也不确定给出的14张图像中有多少张是好的。因此，我们必须阅读所有图像并仅拍摄好图像。
除了棋盘，我们还可以使用圆形网格。在这种情况下，我们必须使用函数`cv.findCirclesGrid()`来找到模式。较少的图像足以使用圆形网格执行相机校准。
找到角点后，可以使用`cv.cornerSubPix()`来提高其精度。我们还可以使用`cv.drawChessboardCorners()`绘制图案。所有这些步骤都包含在以下代码中：

```python
import numpy as np
import cv2 as cv
import glob
# termination criteria
criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)
# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)
objp = np.zeros((6*7,3), np.float32)
objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)
# Arrays to store object points and image points from all the images.
objpoints = [] # 3d point in real world space
imgpoints = [] # 2d points in image plane.
images = glob.glob('*.jpg')
for fname in images:
    img = cv.imread(fname)
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    # Find the chess board corners
    ret, corners = cv.findChessboardCorners(gray, (7,6), None)
    # If found, add object points, image points (after refining them)
    if ret == True:
        objpoints.append(objp)
        corners2 = cv.cornerSubPix(gray,corners, (11,11), (-1,-1), criteria)
        imgpoints.append(corners)
        # Draw and display the corners
        cv.drawChessboardCorners(img, (7,6), corners2, ret)
        cv.imshow('img', img)
        cv.waitKey(500)
cv.destroyAllWindows()
```

一张上面画有图案的图像如下所示：
![](images/calib_pattern.jpg)

## 校准

现在我们有了目标点和图像点，现在可以进行校准了。我们可以使用函数`cv.calibrateCamera()`返回相机矩阵，失真系数，旋转和平移矢量等。

```python
ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)
```

## 不失真

现在，我们可以拍摄图像并对其进行扭曲。OpenCV提供了两种方法来执行此操作。但是，首先，我们可以使用`cv.getOptimalNewCameraMatrix()`基于自由缩放参数来优化相机矩阵。如果缩放参数alpha = 0，则返回具有最少不需要像素的未失真图像。因此，它甚至可能会删除图像角落的一些像素。如果alpha = 1，则所有像素都保留有一些额外的黑色图像。此函数还返回可用于裁剪结果的图像ROI。

因此，我们拍摄一张新图像（在本例中为left12.jpg。这是本章的第一张图像）

```python
img = cv.imread('left12.jpg')
h,  w = img.shape[:2]
newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))
```

### 1.使用cv.undistort()

这是最简单的方法。只需调用该函数并使用上面获得的ROI裁剪结果即可。

```python
# undistort
dst = cv.undistort(img, mtx, dist, None, newcameramtx)
# crop the image
x, y, w, h = roi
dst = dst[y:y+h, x:x+w]
cv.imwrite('calibresult.png', dst)
```

### 2.使用重映射

这样比较困难。首先，找到从扭曲图像到未扭曲图像的映射函数。然后使用重映射功能。

```python
# undistort
mapx, mapy = cv.initUndistortRectifyMap(mtx, dist, None, newcameramtx, (w,h), 5)
dst = cv.remap(img, mapx, mapy, cv.INTER_LINEAR)
# crop the image
x, y, w, h = roi
dst = dst[y:y+h, x:x+w]
cv.imwrite('calibresult.png', dst)
```

尽管如此，两种方法都给出相同的结果。看到下面的结果：
![](images/calib_result.jpg)

您可以看到所有边缘都是笔直的。

现在，您可以使用NumPy中的写入功能（`np.savez`，`np.savetxt`等）存储相机矩阵和失真系数，以备将来使用。

## 重投影误差

重投影误差可以很好地估计找到的参数的精确度。重投影误差越接近零，我们发现的参数越准确。给定固有，失真，旋转和平移矩阵，我们必须首先使用`cv.projectPoints()`将对象点转换为图像点。然后，我们可以计算出通过变换得到的绝对值和拐角发现算法之间的绝对范数。为了找到平均误差，我们计算为所有校准图像计算的误差的算术平均值。

```python
mean_error = 0
for i in xrange(len(objpoints)):
    imgpoints2, _ = cv.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)
    error = cv.norm(imgpoints[i], imgpoints2, cv.NORM_L2)/len(imgpoints2)
    mean_error += error
print( "total error: {}".format(mean_error/len(objpoints)) )
```

# 姿势估计

## 目标

在这个部分，

- 我们将学习利用calib3d模块在图像中创建一些3D效果。

## 基本

这将是一小部分。在上一次相机校准的过程中，您发现了相机矩阵，失真系数等。给定图案图像，我们可以利用以上信息来计算其姿势或物体在空间中的位置，例如其旋转方式，对于平面物体，我们可以假设Z = 0，这样，问题就变成了如何将相机放置在空间中以查看我们的图案图像。因此，如果我们知道对象在空间中的位置，则可以在其中绘制一些2D图以模拟3D效果。让我们来看看如何做。

我们的问题是，我们想在棋盘的第一个角上绘制3D坐标轴（X，Y，Z轴）。X轴为蓝色，Y轴为绿色，Z轴为红色。因此，实际上Z轴应该感觉像它垂直于我们的棋盘平面。

首先，让我们从先前的校准结果中加载相机矩阵和失真系数。

```python
import numpy as np
import cv2 as cv
import glob
# Load previously saved data
with np.load('B.npz') as X:
    mtx, dist, _, _ = [X[i] for i in ('mtx','dist','rvecs','tvecs')]
```

现在，让我们创建一个函数，绘制，该函数将棋盘上的角（使用`cv.findChessboardCorners()`获得）和轴点绘制为一个3D轴。

```python
def draw(img, corners, imgpts):
    corner = tuple(corners[0].ravel())
    img = cv.line(img, corner, tuple(imgpts[0].ravel()), (255,0,0), 5)
    img = cv.line(img, corner, tuple(imgpts[1].ravel()), (0,255,0), 5)
    img = cv.line(img, corner, tuple(imgpts[2].ravel()), (0,0,255), 5)
    return img
```

然后，与前面的情况一样，我们创建终止条件，对象点（棋盘角的3D点）和轴点。轴点是3D空间中用于绘制轴的点。我们绘制长度为3的轴（由于我们基于该尺寸进行校准，因此单位将以国际象棋正方形的尺寸为单位）。因此我们的X轴从（0,0,0）绘制为（3,0,0），因此对于Y轴。对于Z轴，从（0,0,0）绘制为（0,0，-3）。负号表示它被拉向相机。

```python
criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)
objp = np.zeros((6*7,3), np.float32)
objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)
axis = np.float32([[3,0,0], [0,3,0], [0,0,-3]]).reshape(-1,3)
```

现在，像往常一样，我们加载每个图像。搜索7x6网格。如果找到，我们将使用子角像素对其进行优化。然后，使用函数`cv.solvePnPRansac()`计算旋转和平移。一旦有了这些变换矩阵，就可以使用它们将轴点投影到图像平面上。简而言之，我们在图像平面上找到与3D空间中（3,0,0），（0,3,0），（0,0,3）中的每一个相对应的点。一旦获得它们，就可以使用`draw()`函数从第一个角到这些点中的每个点绘制线条。做完!!!

```python
for fname in glob.glob('left*.jpg'):
    img = cv.imread(fname)
    gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)
    ret, corners = cv.findChessboardCorners(gray, (7,6),None)
    if ret == True:
        corners2 = cv.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)
        # Find the rotation and translation vectors.
        ret,rvecs, tvecs = cv.solvePnP(objp, corners2, mtx, dist)
        # project 3D points to image plane
        imgpts, jac = cv.projectPoints(axis, rvecs, tvecs, mtx, dist)
        img = draw(img,corners2,imgpts)
        cv.imshow('img',img)
        k = cv.waitKey(0) & 0xFF
        if k == ord('s'):
            cv.imwrite(fname[:6]+'.png', img)
cv.destroyAllWindows()
```

请参阅下面的一些结果。请注意，每个轴长3个正方形。

![](images/pose_1.jpg)

## 渲染立方体

如果要绘制立方体，请如下修改`draw()`函数和轴点。

修改后的`draw()`函数：

```python
def draw(img, corners, imgpts):
    imgpts = np.int32(imgpts).reshape(-1,2)
    # draw ground floor in green
    img = cv.drawContours(img, [imgpts[:4]],-1,(0,255,0),-3)
    # draw pillars in blue color
    for i,j in zip(range(4),range(4,8)):
        img = cv.line(img, tuple(imgpts[i]), tuple(imgpts[j]),(255),3)
    # draw top layer in red color
    img = cv.drawContours(img, [imgpts[4:]],-1,(0,0,255),3)
    return img
```

修改的轴点。它们是3D空间中多维数据集的8个角：

```python
axis = np.float32([[0,0,0], [0,3,0], [3,3,0], [3,0,0],
                   [0,0,-3],[0,3,-3],[3,3,-3],[3,0,-3] ])
```

查看以下结果：
![](images/pose_2.jpg)

如果您对图形，增强现实等感兴趣，则可以使用OpenGL渲染更复杂的图形。

# 对极几何

## 目标

在这个部分，

- 我们将学习多视图几何的基础知识
- 我们将看到什么是极线，极线，极线约束等。

## 基本概念

当我们使用针孔相机拍摄图像时，我们失去了重要信息，即图像深度。或者图像中的每个点距相机多远，因为它是3D到2D转换。因此，是否能够使用这些摄像机找到深度信息是一个重要的问题。答案是使用不止一台摄像机。在使用两台相机（两只眼睛）的情况下，我们的眼睛以类似的方式工作，这称为立体视觉。因此，让我们看看OpenCV在此字段中提供了什么。

（通过Gary Bradsky 学习OpenCV在该领域有很多信息。）

在深入图像之前，让我们首先了解多视图几何中的一些基本概念。在本节中，我们将讨论对极几何。请参见下图，该图显示了使用两台摄像机拍摄同一场景的图像的基本设置。

![](images/epipolar.jpg)

如果仅使用左摄像机，则找不到与点x对应的3D点在图像中，因为线$OX$上的每个点投影到图像平面上的同一点。但也要考虑正确的图像。现在$OX$线上的不同点投影到不同点（x“）。因此，使用这两个图像，我们可以对正确的3D点进行三角剖分。这就是整个想法。

OX上不同点的投影在右平面上形成一条线（线$l'$）。我们称它为与点x对应的`Epiline`。就是说找到点x在右边的图像上，沿着该主线搜索。它应该在这条线上的某处（以这种方式考虑，可以在其他图像中找到匹配点，而无需搜索整个图像，只需沿着`Epiline`搜索即可。这样可以提供更好的性能和准确性）。这称为对极约束。类似地，所有点在另一幅图像中将具有其对应的`Epiline`。$XOO'$被称为对极面。

$O$和$O'$是相机中心。从上面给出的设置中，您可以看到右摄像机$O'$的投影在左侧图像上可见。它被称为子极。`Epipole`是穿过相机中心和图像平面的线的交点。类似的“是左摄像头的子极。在某些情况下，您将无法在图像中找到子极，它们可能位于图像外部（这意味着一台摄像机看不到另一台）。

所有的`Epilines`都通过其`Epipole`。因此，要找到中心线的位置，我们可以找到许多中心线并找到它们的交点。

因此，我们将重点放在寻找对极线和极线。但是要找到它们，我们还需要另外两种成分，即`基础矩阵（F）`和`基本矩阵（E）`。Essential Matrix包含有关平移和旋转的信息，这些信息在全局坐标中描述了第二个摄像头相对于第一个摄像头的位置。参见下图（图像由Gary Bradsky提供：Learning OpenCV）：
![](images/essential_matrix.jpg)

但是我们更喜欢在像素坐标中进行测量，对吗？基本矩阵除包含有关两个摄像头的内在信息之外，还包含与基本矩阵相同的信息，因此我们可以将两个摄像头的像素坐标关联起来。（如果我们使用的是校正后的图像，并通过除以焦距F= E）。简而言之，基本矩阵F将一个图像中的点映射到另一图像中的线（上）。这是从两个图像的匹配点计算得出的。至少需要8个这样的点才能找到基本矩阵（使用8点算法时）。首选更多点，并使用RANSAC获得更可靠的结果。

## 示例代码

因此，首先我们需要在两个图像之间找到尽可能多的匹配项，以找到基本矩阵。为此，我们将SIFT描述符与基于FLANN的匹配器和比率测试结合使用。

```python
import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt
img1 = cv.imread('myleft.jpg',0)  #queryimage # left image
img2 = cv.imread('myright.jpg',0) #trainimage # right image
sift = cv.SIFT()
# find the keypoints and descriptors with SIFT
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)
# FLANN parameters
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks=50)
flann = cv.FlannBasedMatcher(index_params,search_params)
matches = flann.knnMatch(des1,des2,k=2)
good = []
pts1 = []
pts2 = []
# ratio test as per Lowe's paper
for i,(m,n) in enumerate(matches):
    if m.distance < 0.8*n.distance:
        good.append(m)
        pts2.append(kp2[m.trainIdx].pt)
        pts1.append(kp1[m.queryIdx].pt)
```

现在，我们有了两个图像的最佳匹配列表。让我们找到基本面矩阵。

```python
pts1 = np.int32(pts1)
pts2 = np.int32(pts2)
F, mask = cv.findFundamentalMat(pts1,pts2,cv.FM_LMEDS)
# We select only inlier points
pts1 = pts1[mask.ravel()==1]
pts2 = pts2[mask.ravel()==1]
```

接下来，我们找到Epilines。在第二张图像上绘制与第一张图像中的点相对应的Epilines。因此，在这里提到正确的图像很重要。我们得到了一行线。因此，我们定义了一个新功能来在图像上绘制这些线条。

```python
def drawlines(img1,img2,lines,pts1,pts2):
    ''' img1 - image on which we draw the epilines for the points in img2
        lines - corresponding epilines '''
    r,c = img1.shape
    img1 = cv.cvtColor(img1,cv.COLOR_GRAY2BGR)
    img2 = cv.cvtColor(img2,cv.COLOR_GRAY2BGR)
    for r,pt1,pt2 in zip(lines,pts1,pts2):
        color = tuple(np.random.randint(0,255,3).tolist())
        x0,y0 = map(int, [0, -r[2]/r[1] ])
        x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])
        img1 = cv.line(img1, (x0,y0), (x1,y1), color,1)
        img1 = cv.circle(img1,tuple(pt1),5,color,-1)
        img2 = cv.circle(img2,tuple(pt2),5,color,-1)
    return img1,img2
```

现在，我们在两个图像中都找到了Epiline并将其绘制。

```python
# Find epilines corresponding to points in right image (second image) and
# drawing its lines on left image
lines1 = cv.computeCorrespondEpilines(pts2.reshape(-1,1,2), 2,F)
lines1 = lines1.reshape(-1,3)
img5,img6 = drawlines(img1,img2,lines1,pts1,pts2)
# Find epilines corresponding to points in left image (first image) and
# drawing its lines on right image
lines2 = cv.computeCorrespondEpilines(pts1.reshape(-1,1,2), 1,F)
lines2 = lines2.reshape(-1,3)
img3,img4 = drawlines(img2,img1,lines2,pts2,pts1)
plt.subplot(121),plt.imshow(img5)
plt.subplot(122),plt.imshow(img3)
plt.show()
```

下面是我们得到的结果：
![](images/epiresult.jpg)

您可以在左侧图像中看到所有Epilines都在右侧图像的一点处收敛。那个汇合点就是极点。

为了获得更好的结果，应使用具有良好分辨率和许多非平面点的图像。

## 练习题

- 一个重要的话题是相机的前进。然后，将在两个位置的相同位置看到极点，并且从固定点出现极点。[看到这个讨论](http://answers.opencv.org/question/17912/location-of-epipole/)。
- 基本矩阵估计对匹配，离群值等的质量敏感。如果所有选定的匹配都位于同一平面上，则情况会变得更糟。[检查此讨论](http://answers.opencv.org/question/18125/epilines-not-correct/)。

# 立体图像的深度图

## 目标

在这个环节中

- 我们将学习根据立体图像创建深度图。

## 基本

我们看到了对极约束和其他相关术语等基本概念。我们还看到，如果我们有两个场景相同的图像，我们可以通过直观的方式从中获取深度信息。下面是一张图片和一些简单的数学公式证明了直觉。

![](images/stereo_depth.jpg)
上图包含等效三角形。编写它们的等式将产生以下结果：
$$
disparity=x−x'=Bf/Z
$$
所以它在两个图像之间找到对应的匹配。我们已经看到了外延约束如何使这个操作更快、更准确。一旦找到匹配，它就会找到差距。让我们看看如何使用opencv

## 示例代码

下面的代码片段显示了创建视差图的简单过程。

```python
import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt
imgL = cv.imread('tsukuba_l.png',0)
imgR = cv.imread('tsukuba_r.png',0)
stereo = cv.StereoBM_create(numDisparities=16, blockSize=15)
disparity = stereo.compute(imgL,imgR)
plt.imshow(disparity,'gray')
plt.show()
```

下面的图像包含原始图像（左）及其视差图（右）。如您所见，结果受到高度噪声的污染。通过调整numDisparities和blockSize的值，可以获得更好的结果。
![](images/disparity_map.jpg)
当您熟悉StereoBM时，会有一些参数，可能需要微调参数以获得更好，更平滑的结果。

参数：

- `texture_threshold`：过滤出纹理不足以进行可靠匹配的区域
- `Speckle range and size`：基于块的匹配器通常会在对象边界附近产生“斑点”，匹配的窗口在一侧捕获前景，而在另一侧捕获背景。在此场景中，匹配器似乎还在桌子上投影的纹理中找到小的虚假匹配项。为了消除这些伪像，我们使用由speckle_size和speckle_range参数控制的散斑滤镜对视差图像进行后处理。speckle_size是将视差斑点忽略为“斑点”的像素数。+ speckle_range控制必须将值差异视为同一对象的一部分的程度。
- `Number of disparities`：滑动窗口的像素数。它越大，可见深度的范围就越大，但是需要更多的计算。
- `min_disparity`：从开始搜索的左像素的x位置开始的偏移量。
- `uniqueness_ratio`：另一个后过滤步骤。如果最佳匹配视差不足够好于搜索范围内的所有其他视差，则将像素滤出。如果texture_threshold和散斑过滤仍在通过虚假匹配，则可以尝试进行调整。
- `prefilter_size和prefilter_cap`：预过滤阶段，可标准化图像亮度并增强纹理，以准备块匹配。通常，您不需要调整这些。

